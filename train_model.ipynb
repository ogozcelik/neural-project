{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEE_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGvTY0bFAKjXr5W35jHRha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ogozcelik/neural-project/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htGrkQozPtXe",
        "outputId": "3f6fcb03-09be-45f9-cdc2-89655a37da7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "n6BjZlOQNRmg"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "import datetime, os\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import utils\n",
        "from keras.models import load_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "import h5py\n",
        "import os, sys\n",
        "# from pyimagesearch.cancernet import CancerNet\n",
        "# have your model in another file later"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All the hyperparameters here\n",
        "VAL_SPLIT = 0.15\n",
        "BATCH_SIZE = 64\n",
        "OPT = keras.optimizers.Adam(0.1) # optimizer\n",
        "EARLY_STOPPING_PATIENCE = 5 "
      ],
      "metadata": {
        "id": "nbevqX-xsmor"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read train_ims and test_ims.\n",
        "\n",
        "# convert to tf arrays\n",
        "data_arr = np.asarray(train_ims, np.float32)\n",
        "data_tf = tf.convert_to_tensor(data_np, np.float32)"
      ],
      "metadata": {
        "id": "GrorR8T0wjf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File('/content/drive/MyDrive/EEE443/project/eee443_project_dataset_train.h5', 'r') as f:\n",
        "  # List all groups\n",
        "  print(\"Keys: %s\" % f.keys())\n",
        "  # get the data\n",
        "  train_cap = np.array(list(f[list(f.keys())[0]]))  # captions\n",
        "  train_imid = np.array(list(f[list(f.keys())[1]])) # indices of images\n",
        "  # train_ims = np.array(list(f[list(f.keys())[2]]))  # pretrained feature vector\n",
        "  # train_url = np.array(list(f[list(f.keys())[3]]))  # urls to images\n",
        "  word_code = np.array(list(f[list(f.keys())[4]]))  # vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-4p9b-ygP6w",
        "outputId": "2f8495ef-19d4-4b86-8213-c1f1e084e658"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: <KeysViewHDF5 ['train_cap', 'train_imid', 'train_ims', 'train_url', 'word_code']>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File('/content/drive/MyDrive/EEE443/project/eee443_project_dataset_test.h5', 'r') as f:\n",
        "  # List all groups\n",
        "  print(\"Keys: %s\" % f.keys())\n",
        "  # get the data\n",
        "  test_cap = np.array(list(f[list(f.keys())[0]]))   # captions\n",
        "  test_imid = np.array(list(f[list(f.keys())[1]]))  # indices of images\n",
        "  # test_ims = np.array(list(f[list(f.keys())[2]])) # pretrained feature vector\n",
        "  # test_url = np.array(list(f[list(f.keys())[3]]))   # urls to images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFQzFC6sqcxk",
        "outputId": "faf475b4-ce64-40cb-c4dc-bd8109b82685"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: <KeysViewHDF5 ['test_caps', 'test_imid', 'test_ims', 'test_url']>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct X samples - to save space, give train_imid as train data and map in the load function.\n",
        "train_X = np.array([train_ims[id-1] for id in train_imid])\n",
        "test_X = np.array([test_ims[id-1] for id in test_imid])\n",
        "# shuffle train and test sets and train-test split\n",
        "X_shuffled, Y_shuffled = shuffle(train_X, train_cap, random_state=0)\n",
        "val = int(X_shuffled.shape[0] * VAL_SPLIT)\n",
        "X_train, Y_train = X_shuffled[:-val], Y_shuffled[:-val]\n",
        "X_val, Y_val = X_shuffled[-val:], Y_shuffled[-val:]\n",
        "X_test, Y_test = shuffle(test_X, test_cap, random_state=0)"
      ],
      "metadata": {
        "id": "gJjxdA1ZsVuo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function  # we are not loading from disk now, so no need for tf.data?\n",
        "def load_img_train(img_id, label):  # train_imid and test_imid keep image ids.\n",
        "  img = tf.io.read_file(os.path.join(TRAIN_PATH, str(img_id) + '.jpg'))\n",
        "  img = Image.open(\"gfg.jpg\") # tf.image.decode_png(image, channels=3)\n",
        "  # tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "  image = img.resize((80, 80))  # tf.image.resize(image, (80,80))\n",
        "  image_sequence = image.getdata()  # ??\n",
        "  image_array = np.array(image_sequence)\n",
        "  image_array = np.reshape(image_array,(80,80,3)) # do we need?\n",
        "  return image_array, label"
      ],
      "metadata": {
        "id": "1wgVdc7iQw1R"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def augment(image, label):\n",
        "\t# perform random horizontal and vertical flips\n",
        "\timage = tf.image.random_flip_up_down(image)\n",
        "\timage = tf.image.random_flip_left_right(image)\n",
        "\t# return the image and the label\n",
        "\treturn (image, label)\n",
        " # do better data augmentation"
      ],
      "metadata": {
        "id": "8pI1tjLZxHx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDS = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "trainDS = (trainDS\n",
        "\t.shuffle(X_train.shape[0])\n",
        "\t# .map(load_images, num_parallel_calls=AUTOTUNE)\n",
        "\t.map(augment, num_parallel_calls=AUTOTUNE)\n",
        "\t.cache()\n",
        "\t.batch(BATCH_SIZE)\n",
        "\t.prefetch(AUTOTUNE)\n",
        ")\n",
        "valDS = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
        "valDS = (valDS\n",
        "\t.shuffle(X_val.shape[0])\n",
        "\t# .map(load_images, num_parallel_calls=AUTOTUNE)\n",
        "\t.map(augment, num_parallel_calls=AUTOTUNE)\n",
        "\t.cache()\n",
        "\t.batch(BATCH_SIZE)\n",
        "\t.prefetch(AUTOTUNE)\n",
        ")\n",
        "testDS = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
        "testDS = (testDS\n",
        "\t.shuffle(X_test.shape[0])\n",
        "\t# .map(load_images, num_parallel_calls=AUTOTUNE)\n",
        "\t.map(augment, num_parallel_calls=AUTOTUNE)\n",
        "\t.cache()\n",
        "\t.batch(BATCH_SIZE)\n",
        "\t.prefetch(AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "hCJuJFb2xcbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the inception resnet model\n",
        "inc_resnet = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
        "    include_top=False, weights='imagenet', input_tensor=None,\n",
        "    input_shape=(80,80,3), pooling=None\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrlV9iTVloA7",
        "outputId": "71d8814b-12be-4a25-c099-5f21e23ad708"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 3s 0us/step\n",
            "219070464/219055592 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take until the chosen layer. make it trainable.\n",
        "hidden_layer = inc_resnet.layers[-1].output\n",
        "cnn_model = tf.keras.Model(inc_resnet.input, hidden_layer)"
      ],
      "metadata": {
        "id": "U7sxjvPC3UsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tdqm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqTohwThzxCk",
        "outputId": "473e56aa-8540-4f2f-84f6-f668b2f50454"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tdqm\n",
            "  Downloading tdqm-0.0.1.tar.gz (1.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tdqm) (4.62.3)\n",
            "Building wheels for collected packages: tdqm\n",
            "  Building wheel for tdqm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tdqm: filename=tdqm-0.0.1-py3-none-any.whl size=1323 sha256=94210c8d57d3c690f249178b5be02726d8db7b8967999e199a8b69b9e6bcd2db\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/f0/d9/9fa5ff78c0f9d5a0a427bbbb4893c283520ddfccb885ea2205\n",
            "Successfully built tdqm\n",
            "Installing collected packages: tdqm\n",
            "Successfully installed tdqm-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique images\n",
        "encode_train = (train_ims) # train_ims are all sorted. Save with order ids.\n",
        "\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(train_ims) # ??\n",
        "image_dataset = image_dataset.map(num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
        "  # load_image,\n",
        "\n",
        "for img in tdqm(image_dataset):\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf in batch_features:\n",
        "    path = os.path.join(FEATURES_MAP)\n",
        "    np.save(path_of_feature, bf.numpy())"
      ],
      "metadata": {
        "id": "T6kDzumt2-YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize an early stopping callback to prevent the model from\n",
        "# overfitting\n",
        "early_stop = EarlyStopping(\n",
        "\tmonitor=\"val_loss\",\n",
        "\tpatience=EARLY_STOPPING_PATIENCE,\n",
        "\trestore_best_weights=True)"
      ],
      "metadata": {
        "id": "rp1wQvqBzxZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model for visualizing the activations\n",
        "layer_outputs = [layer.output for layer in inc_resnet.layers[:20]] \n",
        "activation_model = keras.models.Model(inputs=inc_resnet.input, outputs=layer_outputs)\n",
        "activations = activation_model.predict(img) "
      ],
      "metadata": {
        "id": "LfurKoLlYsjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot all the activations\n",
        "layer_names = []\n",
        "for layer in classifier.layers[:12]:\n",
        "    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n",
        "    \n",
        "images_per_row = 16\n",
        "â€‹\n",
        "for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
        "    n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
        "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
        "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
        "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
        "                         row * size : (row + 1) * size] = channel_image\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "    # choose until which layer you want to take"
      ],
      "metadata": {
        "id": "f3i_xeZXarUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H = model.fit(\n",
        "\tx=trainDS,\n",
        "\tvalidation_data=valDS,\n",
        "\tclass_weight=classWeight,\n",
        "\tepochs=config.NUM_EPOCHS,\n",
        "\tcallbacks=[es],\n",
        "\tverbose=1)"
      ],
      "metadata": {
        "id": "oOFSWbXYy8GP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}